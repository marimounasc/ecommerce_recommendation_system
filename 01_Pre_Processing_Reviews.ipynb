{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avaliacoes_df = pd.read_csv('/Users/Mariana Nascimento/Modelo_Recomendacao_ECommerce/results/avaliacoes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = avaliacoes_df['texto_da_avaliacao']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Limpeza de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover acentos\n",
    "def remove_accents(text):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', text)\n",
    "    return ''.join([char for char in nfkd_form if not unicodedata.combining(char)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de limpeza de texto\n",
    "def clean_text(text):\n",
    "    text = remove_accents(text)  # Remover acentos\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remover tags HTML\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)  # Remover caracteres especiais, exceto espaços\n",
    "    text = text.lower()  # Converter para minúsculas\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.apply(remove_accents)\n",
    "reviews = reviews.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenização é o processo de dividir um texto em unidades menores chamadas tokens. Esses tokens podem ser palavras, frases ou até mesmo caracteres, dependendo do nível de granularidade desejado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"Processamento de Linguagem Natural com Python é ótimo. NLTK facilita bastante.\"\n",
    "palavras = word_tokenize(texto, language='portuguese')\n",
    "sentencas = sent_tokenize(texto, language='portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Remoção de 'stop words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    # Tokenizar o texto em palavras\n",
    "    words = word_tokenize(text, language='portuguese')\n",
    "    # Filtrar as palavras que não estão na lista de stop words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Reconstruir o texto a partir das palavras filtradas\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Entidades (Não sera usado nesse modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def extract_entities(text):\n",
    "    # Processar o texto com o modelo SpaCy\n",
    "    doc = nlp(text)\n",
    "    # Extrair entidades nomeadas\n",
    "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "texto_exemplo = \"O iPhone 12 é um excelente smartphone da Apple. Eu o comprei na loja da Apple em São Paulo.\"\n",
    "entidades = extract_entities(texto_exemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o stemmer para português\n",
    "stemmer = RSLPStemmer()\n",
    "\n",
    "def apply_stemming(text):\n",
    "    # Tokenizar o texto em palavras\n",
    "    words = word_tokenize(text, language='portuguese')\n",
    "    # Aplicar o stemmer a cada palavra\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    # Reconstruir o texto a partir das palavras stemmizadas\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.apply(apply_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Embeddings contextuais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o tokenizador e o modelo BERT pré-treinado\n",
    "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "model = BertModel.from_pretrained('neuralmind/bert-base-portuguese-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para vetorizar as reviews usando BERT\n",
    "def vectorize_reviews(reviews):\n",
    "    # Tokenizar as reviews\n",
    "    inputs = tokenizer(reviews, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    # Passar as reviews pelo modelo BERT para obter as embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Obter a saída da camada de embeddings oculta\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = vectorize_reviews(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_list = embeddings.tolist()\n",
    "\n",
    "# Adicionar os embeddings como uma nova coluna no dataframe original\n",
    "avaliacoes_df['embeddings'] = pd.Series(embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avaliacoes_df.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
